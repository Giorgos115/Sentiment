{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Necesssary Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import yfinance as yf\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import pysentiment2 as ps\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to tokenize\n",
    "def process_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    remove_punc = str.maketrans('','', string.punctuation + string.digits)\n",
    "    filtered_tokens = [word.translate(remove_punc) for word in tokens if word.isalpha() and word not in stopwords.words('english')]\n",
    "    filtered_tokens = [word for word in filtered_tokens if word]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to calculate tone\n",
    "def calculate_tone(document, positive_words, negative_words):\n",
    "    word_freq = Counter(process_text(document))\n",
    "    positive_count = sum(word_freq[word] for word in positive_words if word in word_freq)\n",
    "    negative_count = sum(word_freq[word] for word in negative_words if word in word_freq)\n",
    "    tone = (positive_count - negative_count) / len(word_freq)\n",
    "    return tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the score for a document using pysentiment2\n",
    "def get_hiv4_score(text):\n",
    "    hiv4 = ps.HIV4()\n",
    "    tokens = hiv4.tokenize(text)\n",
    "    return hiv4.get_score(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LM list\n",
    "lm_dictionary_path = \"/Loughran-McDonald_MasterDictionary_1993-2021.csv\"\n",
    "lm_dictionary = pd.read_csv(lm_dictionary_path)\n",
    "\n",
    "# Extract lists of positive and negative words\n",
    "positive_words_lm = set(lm_dictionary[lm_dictionary['Positive'] != 0]['Word'].str.lower())\n",
    "negative_words_lm = set(lm_dictionary[lm_dictionary['Negative'] != 0]['Word'].str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FOMC documents and dates\n",
    "fomc_documents = []\n",
    "fomc_dates = []\n",
    "path_to_fomc_docs = \"/Text Files\"\n",
    "for file_name in os.listdir(path_to_fomc_docs):\n",
    "    date_str = file_name.split('.')[0]\n",
    "    date_obj = datetime.strptime(date_str, '%Y%m%d')\n",
    "    fomc_dates.append(date_obj)\n",
    "    with open(os.path.join(path_to_fomc_docs, file_name), 'r', encoding='ISO-8859-1') as file:\n",
    "        fomc_documents.append(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dates and start counter\n",
    "fomc_dates.sort()\n",
    "overall_word_freq = Counter()\n",
    "\n",
    "# Process and count words in each document\n",
    "for doc in fomc_documents:\n",
    "    processed_text = process_text(doc)\n",
    "    overall_word_freq.update(processed_text)\n",
    "    \n",
    "# Convert to DataFrame and find the most influential unigrams\n",
    "df_word_freq = pd.DataFrame(overall_word_freq.items(), columns=['Word', 'Frequency'])\n",
    "df_word_freq_sorted = df_word_freq.sort_values(by='Frequency', ascending=False).reset_index(drop=True)\n",
    "print(df_word_freq_sorted.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate tone, sort by date\n",
    "tones_lm = [calculate_tone(doc, positive_words_lm, negative_words_lm) for doc in fomc_documents]\n",
    "fomc_dates, tones_lm = zip(*sorted(zip(fomc_dates, tones_lm)))\n",
    "df_tones = pd.DataFrame({'Date': fomc_dates, 'Tone': tones_lm})\n",
    "df_tones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting tone of FOMC documents over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_tones['Date'], df_tones['Tone'], marker='o', linestyle='-', color='blue')\n",
    "plt.title('Tone of Federal Reserve Policy Statements (2008-2018)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Tone')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datetime and group by year\n",
    "df_tones['Date'] = pd.to_datetime(df_tones['Date'])\n",
    "df_tones['Year'] = df_tones['Date'].dt.year\n",
    "average_tone_per_year = df_tones.groupby('Year')['Tone'].mean().reset_index()\n",
    "print(\"Average Tone per Year:\")\n",
    "print(average_tone_per_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(average_tone_per_year['Year'], average_tone_per_year['Tone'], marker='o', linestyle='-', color='green')\n",
    "plt.title('Average Tone of Federal Reserve Policy Statements by Year (2008-2018)')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Tone')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to couunt positive and negative words in LM\n",
    "def count_lm_words(document, positive_words, negative_words):\n",
    "    word_freq = Counter(process_text(document))\n",
    "    positive_count = sum(word_freq[word] for word in positive_words if word in word_freq)\n",
    "    negative_count = sum(word_freq[word] for word in negative_words if word in word_freq)\n",
    "    return positive_count, negative_count\n",
    "\n",
    "# Make counters for positive and negative words\n",
    "lm_positive_counts = []\n",
    "lm_negative_counts = []\n",
    "harvard_positive_counts = []\n",
    "harvard_negative_counts = []\n",
    "\n",
    "# Loop for documents\n",
    "for doc in fomc_documents:\n",
    "    lm_pos_count, lm_neg_count = count_lm_words(doc, positive_words_lm, negative_words_lm)\n",
    "    lm_positive_counts.append(lm_pos_count)\n",
    "    lm_negative_counts.append(lm_neg_count)\n",
    "    score = get_hiv4_score(doc)\n",
    "    harvard_positive_counts.append(score['Positive'])\n",
    "    harvard_negative_counts.append(score['Negative'])\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "df_comparison = pd.DataFrame({\n",
    "    'Document Date': fomc_dates,\n",
    "    'LM Positive': lm_positive_counts,\n",
    "    'LM Negative': lm_negative_counts,\n",
    "    'Harvard Positive': harvard_positive_counts,\n",
    "    'Harvard Negative': harvard_negative_counts\n",
    "})\n",
    "print(df_comparison.head(81))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Negative Word Counts from LM and Harvard\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(df_comparison['Document Date'], df_comparison['LM Negative'], label='LM Negative', marker='o', linestyle='-', color='red')\n",
    "plt.plot(df_comparison['Document Date'], df_comparison['Harvard Negative'], label='Harvard Negative', marker='x', linestyle='--', color='black')\n",
    "plt.title('Comparison of Negative Word Counts (LM vs Harvard)')\n",
    "plt.xlabel('Document Date')\n",
    "plt.ylabel('Count of Negative Words')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Positive Word Counts from LM and Harvard\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(df_comparison['Document Date'], df_comparison['LM Positive'], label='LM Positive', marker='o', linestyle='-', color='blue')\n",
    "plt.plot(df_comparison['Document Date'], df_comparison['Harvard Positive'], label='Harvard Positive', marker='x', linestyle='--', color='green')\n",
    "plt.title('Comparison of Positive Word Counts (LM vs Harvard)')\n",
    "plt.xlabel('Document Date')\n",
    "plt.ylabel('Count of Positive Words')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LM word list\n",
    "lm_dictionary = pd.read_csv(lm_dictionary_path)\n",
    "positive_words_lm = set(lm_dictionary[lm_dictionary['Positive'] != 0]['Word'].str.lower())\n",
    "negative_words_lm = set(lm_dictionary[lm_dictionary['Negative'] != 0]['Word'].str.lower())\n",
    "\n",
    "def process_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    remove_punc = str.maketrans('', '', string.punctuation + string.digits)\n",
    "    filtered_tokens = [word.translate(remove_punc) for word in tokens if word.isalpha()]\n",
    "    filtered_tokens = [word for word in filtered_tokens if word not in stopwords.words('english')]\n",
    "    return filtered_tokens\n",
    "\n",
    "all_words_freq = Counter()\n",
    "\n",
    "# Process each document in the folder and count all words\n",
    "for file_name in os.listdir(path_to_fomc_docs):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(path_to_fomc_docs, file_name)\n",
    "        with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "            text = file.read()\n",
    "            tokens = process_text(text)\n",
    "            all_words_freq.update(tokens)\n",
    "\n",
    "# Filter into positive/negative sets, sort frequencies\n",
    "positive_word_freq = {word: freq for word, freq in all_words_freq.items() if word in positive_words_lm}\n",
    "negative_word_freq = {word: freq for word, freq in all_words_freq.items() if word in negative_words_lm}\n",
    "\n",
    "sorted_positive_words = sorted(positive_word_freq.items(), key=lambda item: item[1], reverse=True)\n",
    "sorted_negative_words = sorted(negative_word_freq.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "top_positive_words = sorted_positive_words[:10]\n",
    "top_negative_words = sorted_negative_words[:10]\n",
    "\n",
    "positive_word_freq = {word: freq for word, freq in overall_word_freq.items() if word in positive_words_lm}\n",
    "negative_word_freq = {word: freq for word, freq in overall_word_freq.items() if word in negative_words_lm}\n",
    "\n",
    "top_positive_words = pd.DataFrame(sorted(positive_word_freq.items(), key=lambda x: x[1], reverse=True)[:10], columns=['Word', 'Frequency'])\n",
    "top_negative_words = pd.DataFrame(sorted(negative_word_freq.items(), key=lambda x: x[1], reverse=True)[:10], columns=['Word', 'Frequency'])\n",
    "\n",
    "# Display the dataframes as tables\n",
    "print(\"Top 10 Positive Words:\")\n",
    "print(top_positive_words.to_string(index=False))\n",
    "print(\"\\nTop 10 Negative Words:\")\n",
    "print(top_negative_words.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "harvard_scores = []\n",
    "\n",
    "# Loop for documents\n",
    "for doc in fomc_documents:\n",
    "    score = get_hiv4_score(doc)\n",
    "    harvard_scores.append(score)\n",
    "for date, score in zip(fomc_dates, harvard_scores):\n",
    "    print(f\"Document Date: {date}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dates into datetime\n",
    "dates = [datetime.strptime(str(date), '%Y-%m-%d %H:%M:%S') for date in fomc_dates]\n",
    "fomc_scores = [get_hiv4_score(doc) for doc in fomc_documents]\n",
    "polarity_scores = [score['Polarity'] for score in fomc_scores]\n",
    "\n",
    "\n",
    "# Sort and plot\n",
    "dates, polarity_scores = zip(*sorted(zip(dates, polarity_scores)))\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot_date(dates, polarity_scores, linestyle='solid', marker=None)\n",
    "plt.title('Polarity Scores Over Time', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Polarity Score', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_freq = Counter()\n",
    "\n",
    "# Process documents\n",
    "for doc in fomc_documents:\n",
    "    tokens = process_text(doc)\n",
    "    all_words_freq.update(tokens)\n",
    "\n",
    "harvard_sentiment_word_freq = Counter()\n",
    "\n",
    "for doc in fomc_documents:\n",
    "    tokens = process_text(doc)\n",
    "    score = get_hiv4_score(doc)\n",
    "    for token in tokens:\n",
    "        if score['Polarity'] > 0:\n",
    "            harvard_sentiment_word_freq[token] += score['Polarity']\n",
    "        elif score['Polarity'] < 0:\n",
    "            harvard_sentiment_word_freq[token] -= score['Polarity']\n",
    "\n",
    "# But you can now sort this to get the words that have the highest and lowest scores\n",
    "sorted_sentiment_words = sorted(harvard_sentiment_word_freq.items(), key=lambda item: item[1], reverse=True)\n",
    "print(sorted_sentiment_words[:10]) \n",
    "print(sorted_sentiment_words[-10:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "positive_word_count = Counter()\n",
    "negative_word_count = Counter()\n",
    "\n",
    "# Loop for documents\n",
    "for doc in fomc_documents:\n",
    "    tokens = process_text(doc)\n",
    "    for token in tokens:\n",
    "        if len(token) > 1:  \n",
    "            score = sia.polarity_scores(token)\n",
    "            if score['compound'] > 0.1: \n",
    "                positive_word_count[token] += 1\n",
    "            elif score['compound'] < -0.1: \n",
    "                negative_word_count[token] += 1\n",
    "\n",
    "top_positive_words = positive_word_count.most_common(10)\n",
    "top_negative_words = negative_word_count.most_common(10)\n",
    "\n",
    "# Convert counts to DataFrames\n",
    "df_top_positive_words = pd.DataFrame(top_positive_words, columns=['Word', 'Count'])\n",
    "df_top_negative_words = pd.DataFrame(top_negative_words, columns=['Word', 'Count'])\n",
    "\n",
    "print(\"Top Positive Words:\")\n",
    "print(df_top_positive_words)\n",
    "print(\"\\nTop Negative Words:\")\n",
    "print(df_top_negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LM\n",
    "# Fetch S&P 500 data from 2008-2017\n",
    "sp500 = yf.download('^GSPC', start='2008-01-01', end='2018-01-01')\n",
    "\n",
    "# Calculate returns, format dates\n",
    "sp500['Returns'] = sp500['Adj Close'].pct_change()\n",
    "sp500_monthly_returns = sp500['Returns'].resample('M').agg(lambda x: (x + 1).prod() - 1)\n",
    "sp500_monthly_returns = sp500_monthly_returns.reset_index()\n",
    "df_tones['Date'] = pd.to_datetime(df_tones['Date'])\n",
    "sp500_monthly_returns['Date'] = pd.to_datetime(sp500_monthly_returns['Date'])\n",
    "\n",
    "# Merge data\n",
    "merged_data = pd.merge_asof(df_tones.sort_values('Date'), sp500_monthly_returns.sort_values('Date'), on='Date', direction='nearest')\n",
    "merged_data.dropna(subset=['Returns'], inplace=True)\n",
    "\n",
    "# Regression analysis\n",
    "X = sm.add_constant(merged_data['Tone'])\n",
    "Y = merged_data['Returns']\n",
    "\n",
    "model = sm.OLS(Y, X).fit()\n",
    "predictions = model.predict(X) \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HARVARD\n",
    "# Define function to get the score for a document using pysentiment2\n",
    "def get_hiv4_score(text):\n",
    "    hiv4 = ps.HIV4()\n",
    "    tokens = hiv4.tokenize(text)\n",
    "    return hiv4.get_score(tokens)\n",
    "\n",
    "# Load FOMC documents and their datesx\n",
    "fomc_documents = []\n",
    "fomc_dates = []\n",
    "path_to_fomc_docs = \"/Text Files\"\n",
    "for file_name in os.listdir(path_to_fomc_docs):\n",
    "    date_str = file_name.split('.')[0]\n",
    "    date_obj = pd.to_datetime(date_str, format='%Y%m%d')\n",
    "    fomc_dates.append(date_obj)\n",
    "    with open(os.path.join(path_to_fomc_docs, file_name), 'r', encoding='ISO-8859-1') as file:\n",
    "        fomc_documents.append(file.read())\n",
    "\n",
    "fomc_scores = [get_hiv4_score(doc) for doc in fomc_documents]\n",
    "harvard_polarity_scores = [score['Polarity'] for score in fomc_scores]\n",
    "\n",
    "# Fetch S&P 500 data, calculate returns, format\n",
    "sp500 = yf.download('^GSPC', start='2008-01-01', end='2018-01-01')\n",
    "sp500['Returns'] = sp500['Adj Close'].pct_change()\n",
    "sp500_monthly_returns = sp500['Returns'].resample('M').agg(lambda x: (x + 1).prod() - 1)\n",
    "sp500_monthly_returns = sp500_monthly_returns.reset_index()\n",
    "sp500_monthly_returns['Date'] = pd.to_datetime(sp500_monthly_returns['Date'])\n",
    "\n",
    "# Align polarity with S&P 500 data\n",
    "aligned_data_harvard = pd.DataFrame({\n",
    "    'Date': fomc_dates,\n",
    "    'Harvard_Polarity': harvard_polarity_scores\n",
    "})\n",
    "\n",
    "# Merge data\n",
    "merged_data_harvard = pd.merge_asof(\n",
    "    aligned_data_harvard.sort_values('Date'), \n",
    "    sp500_monthly_returns.sort_values('Date'), \n",
    "    on='Date', \n",
    "    direction='nearest'\n",
    ")\n",
    "merged_data_harvard.dropna(subset=['Returns'], inplace=True)\n",
    "\n",
    "# Regression analysis\n",
    "X_harvard = sm.add_constant(merged_data_harvard['Harvard_Polarity']) \n",
    "Y_harvard = merged_data_harvard['Returns']\n",
    "\n",
    "model_harvard = sm.OLS(Y_harvard, X_harvard).fit()\n",
    "print(\"Harvard Dictionary Polarity Regression Summary:\")\n",
    "print(model_harvard.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
